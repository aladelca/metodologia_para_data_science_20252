{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94766bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pandas seaborn matplotlib numpy statsmodels\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"../src/preprocessing/data/raw/raw_stock_data.parquet\")\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e19d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dibujar los √∫ltimos 10 a√±os\n",
    "\n",
    "data = data.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b25e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_years = data[data[\"Date\"]>='2000-01-01']\n",
    "\n",
    "sns.lineplot(x=\"Date\", y=\"Close\", data=last_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe294a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = data.set_index(\"Date\")[\"Close\"]\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomposici√≥n de series de tiempo\n",
    "import statsmodels\n",
    "decomposition = statsmodels.tsa.seasonal.seasonal_decompose(ts, period=252, model='additive')\n",
    "decomposition.plot()\n",
    "plt.show()\n",
    "\n",
    "decomposition.trend.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fffd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition.seasonal.plot()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df82c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition.resid.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "import pandas as pd\n",
    "\n",
    "# Funci√≥n para realizar e interpretar el test ADF\n",
    "def adf_test(timeseries, title=''):\n",
    "    \"\"\"\n",
    "    Realiza el test de Dickey-Fuller Aumentado\n",
    "    H0: La serie tiene ra√≠z unitaria (no es estacionaria)\n",
    "    H1: La serie es estacionaria\n",
    "    \"\"\"\n",
    "    print(f'Resultados del Test de Dickey-Fuller para {title}:')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Realizar el test\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    \n",
    "    # Organizar resultados\n",
    "    dfoutput = pd.Series(dftest[0:4], index=[\n",
    "        'Estad√≠stico ADF',\n",
    "        'p-value',\n",
    "        'Lags utilizados',\n",
    "        'N√∫mero de observaciones'\n",
    "    ])\n",
    "    \n",
    "    # Valores cr√≠ticos\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput[f'Valor cr√≠tico ({key})'] = value\n",
    "    \n",
    "    print(dfoutput)\n",
    "    print()\n",
    "    \n",
    "    # Interpretaci√≥n\n",
    "    if dftest[1] <= 0.05:\n",
    "        print(\"‚úÖ RESULTADO: Rechazamos H0 - La serie ES ESTACIONARIA\")\n",
    "        print(f\"   p-value ({dftest[1]:.6f}) <= 0.05\")\n",
    "    else:\n",
    "        print(\"‚ùå RESULTADO: No rechazamos H0 - La serie NO ES ESTACIONARIA\")\n",
    "        print(f\"   p-value ({dftest[1]:.6f}) > 0.05\")\n",
    "    \n",
    "    print()\n",
    "    return dftest\n",
    "\n",
    "\n",
    "adf_original = adf_test(ts, 'Serie Original')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a79de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = ts.diff().dropna()\n",
    "adf_diff = adf_test(dts, 'Serie Diferenciada')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "decomposition = statsmodels.tsa.seasonal.seasonal_decompose(dts, period=252, model='additive')\n",
    "decomposition.plot()\n",
    "plt.show()\n",
    "\n",
    "decomposition.trend.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gr√°fico de Autocorrelaci√≥n Parcial (PACF)\n",
    "def grafico_pacf(serie, lags=20, titulo=\"\"):\n",
    "    \"\"\"\n",
    "    Crea gr√°fico de autocorrelaci√≥n parcial\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_pacf(serie, lags=lags, alpha=0.05, method='ywm')\n",
    "    plt.title(f'Autocorrelaci√≥n Parcial (PACF) - {titulo}')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('PACF')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Gr√°fico de Autocorrelaci√≥n (ACF)\n",
    "def grafico_acf(serie, lags=20, titulo=\"\"):\n",
    "    \"\"\"\n",
    "    Crea gr√°fico de autocorrelaci√≥n\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_acf(serie, lags=lags, alpha=0.05)\n",
    "    plt.title(f'Autocorrelaci√≥n (ACF) - {titulo}')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('ACF')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Usar las funciones\n",
    "grafico_pacf(dts, lags=30, titulo=\"Serie Diferenciada\")\n",
    "grafico_acf(dts, lags=30, titulo=\"Serie Diferenciada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import pacf, acf\n",
    "\n",
    "def pacf_con_interpretacion(serie, lags=20, titulo=\"\"):\n",
    "    \"\"\"\n",
    "    PACF con interpretaci√≥n autom√°tica para determinar orden AR\n",
    "    \"\"\"\n",
    "    # Calcular PACF\n",
    "    pacf_values, confint = pacf(serie, nlags=lags, alpha=0.05)\n",
    "    \n",
    "    # Encontrar lags significativos\n",
    "    lags_significativos = []\n",
    "    for i in range(1, len(pacf_values)):\n",
    "        if abs(pacf_values[i]) > abs(confint[i, 1] - pacf_values[i]):\n",
    "            lags_significativos.append(i)\n",
    "    \n",
    "    # Graficar\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_pacf(serie, lags=lags, alpha=0.05, method='ywm')\n",
    "    plt.title(f'PACF con Interpretaci√≥n - {titulo}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Marcar lags significativos\n",
    "    for lag in lags_significativos:\n",
    "        plt.axvline(x=lag, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Interpretaci√≥n\n",
    "    print(f\"üìä INTERPRETACI√ìN PACF - {titulo}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Lags significativos: {lags_significativos}\")\n",
    "    \n",
    "    if len(lags_significativos) > 0:\n",
    "        p_sugerido = max(lags_significativos)\n",
    "        print(f\"Orden AR sugerido (p): {p_sugerido}\")\n",
    "    else:\n",
    "        print(\"No se detectaron lags significativos\")\n",
    "        print(\"Orden AR sugerido (p): 0\")\n",
    "    \n",
    "    return lags_significativos\n",
    "\n",
    "# Usar con interpretaci√≥n\n",
    "lags_sig = pacf_con_interpretacion(ts.diff().dropna(), lags=25, titulo=\"Primera Diferencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be19338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esquema de validaci√≥n\n",
    "\n",
    "train = dts.loc[:'2023-12-31']\n",
    "test = dts.loc['2024-01-01':]\n",
    "\n",
    "print(train.index.max())\n",
    "print(test.index.min())\n",
    "\n",
    "\n",
    "len(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def crear_modelo_arima(serie, orden=(1,1,1)):\n",
    "    \"\"\"\n",
    "    Crea y ajusta un modelo ARIMA\n",
    "    orden = (p, d, q) donde:\n",
    "    p = orden AR (autoregresivo)\n",
    "    d = orden de diferenciaci√≥n\n",
    "    q = orden MA (media m√≥vil)\n",
    "    \"\"\"\n",
    "    print(f\"üîß Creando modelo ARIMA{orden}\")\n",
    "    \n",
    "    # Crear el modelo\n",
    "    modelo = ARIMA(serie, order=orden)\n",
    "    \n",
    "    # Ajustar el modelo\n",
    "    modelo_ajustado = modelo.fit()\n",
    "    \n",
    "    # Mostrar resumen\n",
    "    print(modelo_ajustado.summary())\n",
    "    \n",
    "    return modelo_ajustado\n",
    "\n",
    "# Ejemplo de uso\n",
    "modelo = crear_modelo_arima(train.loc[\"2021-01-01\":], orden=(25,0,10)) # Diversas pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = modelo.forecast(steps=426)\n",
    "conf_int = modelo.get_forecast(steps=426).conf_int()\n",
    "\n",
    "print(  len(forecast))\n",
    "print(len(test))\n",
    "\n",
    "\n",
    "df_forecast = pd.DataFrame(forecast)\n",
    "df_forecast.index = test.index\n",
    "\n",
    "\n",
    "sns.lineplot(df_forecast.loc[:\"2024-01-31\"])\n",
    "sns.lineplot(test.loc[:\"2024-01-31\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ea3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def auto_arima_manual(serie, max_p=30, max_d=0, max_q=30, criterio='aic'):\n",
    "    \"\"\"\n",
    "    Implementaci√≥n manual de auto_arima usando solo statsmodels\n",
    "    \"\"\"\n",
    "    print(\"üîç B√∫squeda autom√°tica de par√°metros ARIMA (versi√≥n manual)...\")\n",
    "    \n",
    "    # Generar todas las combinaciones\n",
    "    p_values = range(0, max_p + 1)\n",
    "    d_values = range(0, max_d + 1)  \n",
    "    q_values = range(0, max_q + 1)\n",
    "    \n",
    "    best_score = float(\"inf\")\n",
    "    best_order = None\n",
    "    best_model = None\n",
    "    resultados = []\n",
    "    \n",
    "    for p, d, q in itertools.product(p_values, d_values, q_values):\n",
    "        try:\n",
    "            # Crear y ajustar modelo\n",
    "            modelo = ARIMA(serie, order=(p, d, q))\n",
    "            modelo_ajustado = modelo.fit()\n",
    "            \n",
    "            # Obtener criterio de informaci√≥n\n",
    "            if criterio == 'aic':\n",
    "                score = modelo_ajustado.aic\n",
    "            elif criterio == 'bic':\n",
    "                score = modelo_ajustado.bic\n",
    "            else:\n",
    "                score = modelo_ajustado.aic\n",
    "                \n",
    "            resultados.append({\n",
    "                'Orden': (p, d, q),\n",
    "                'AIC': modelo_ajustado.aic,\n",
    "                'BIC': modelo_ajustado.bic,\n",
    "                'LogLik': modelo_ajustado.llf\n",
    "            })\n",
    "            \n",
    "            print(f\"ARIMA({p},{d},{q}) - AIC: {modelo_ajustado.aic:.2f}, BIC: {modelo_ajustado.bic:.2f}\")\n",
    "            \n",
    "            # Actualizar mejor modelo\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_order = (p, d, q)\n",
    "                best_model = modelo_ajustado\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error con ARIMA({p},{d},{q}): {str(e)[:50]}...\")\n",
    "            continue\n",
    "    \n",
    "    # Crear DataFrame con resultados\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    df_resultados = df_resultados.sort_values('AIC')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Mejor modelo: ARIMA{best_order}\")\n",
    "    print(f\"   {criterio.upper()}: {best_score:.2f}\")\n",
    "    \n",
    "    return best_model, best_order, df_resultados\n",
    "\n",
    "# Usar la funci√≥n\n",
    "mejor_modelo, mejor_orden, tabla_resultados = auto_arima_manual(dts, max_p=30, max_d=0, max_q=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[\"2021-01-01\":].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def crear_variable_exogena_segura(serie, periodos_eventos, nombre_evento=\"evento\"):\n",
    "    \"\"\"\n",
    "    Versi√≥n segura que maneja zonas horarias autom√°ticamente\n",
    "    \"\"\"\n",
    "    print(f\"üîß Creando variable ex√≥gena para {nombre_evento} (versi√≥n segura)...\")\n",
    "    \n",
    "    # Normalizar serie\n",
    "    serie_trabajo = serie.copy()\n",
    "    \n",
    "    # Manejar zona horaria de la serie\n",
    "    if hasattr(serie_trabajo.index, 'tz') and serie_trabajo.index.tz is not None:\n",
    "        print(f\"Convirtiendo de zona horaria: {serie_trabajo.index.tz}\")\n",
    "        serie_trabajo.index = serie_trabajo.index.tz_convert('UTC').tz_localize(None)\n",
    "    else:\n",
    "        serie_trabajo.index = pd.to_datetime(serie_trabajo.index, utc=False).tz_localize(None)\n",
    "    \n",
    "    # Crear variable ex√≥gena\n",
    "    variable_exogena = pd.Series(0, index=serie_trabajo.index, name=f'{nombre_evento}_flag')\n",
    "    \n",
    "    eventos_marcados = 0\n",
    "    for i, (fecha_inicio, fecha_fin) in enumerate(periodos_eventos):\n",
    "        try:\n",
    "            # Normalizar fechas de eventos\n",
    "            inicio = pd.to_datetime(fecha_inicio, utc=False).tz_localize(None)\n",
    "            fin = pd.to_datetime(fecha_fin, utc=False).tz_localize(None)\n",
    "            \n",
    "            # Verificar que las fechas est√°n en el rango de la serie\n",
    "            if inicio > serie_trabajo.index.max() or fin < serie_trabajo.index.min():\n",
    "                print(f\"  ‚ö†Ô∏è Per√≠odo {i+1} fuera del rango de datos: {inicio} a {fin}\")\n",
    "                continue\n",
    "            \n",
    "            # Marcar per√≠odo\n",
    "            mask = (serie_trabajo.index >= inicio) & (serie_trabajo.index <= fin)\n",
    "            variable_exogena[mask] = 1\n",
    "            \n",
    "            puntos_marcados = mask.sum()\n",
    "            eventos_marcados += puntos_marcados\n",
    "            print(f\"  ‚úÖ Per√≠odo {i+1}: {inicio.date()} a {fin.date()} - {puntos_marcados} puntos\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error en per√≠odo {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚úÖ Total marcado: {eventos_marcados}/{len(serie_trabajo)} puntos\")\n",
    "    \n",
    "    return variable_exogena, serie_trabajo\n",
    "\n",
    "# Usar funci√≥n segura\n",
    "\n",
    "\n",
    "# Definir per√≠odos de eventos importantes\n",
    "periodos = [\n",
    "    ['2022-02-01', \"2022-04-01\"],\n",
    "    [\"2023-03-01\", \"2023-05-01\"],\n",
    "    [\"2024-02-01\", \"2024-04-01\"],\n",
    "    [\"2025-02-01\", \"2025-04-01\"],  # Este per√≠odo podr√≠a estar fuera del rango\n",
    "]\n",
    "\n",
    "# Crear variable ex√≥gena\n",
    "evento_flag_seguro, dts_seguro = crear_variable_exogena_segura(dts, periodos, \"evento_importante\")\n",
    "\n",
    "\n",
    "\n",
    "dts_seguro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_sarimax_corregido(serie, periodos_eventos, nombre_serie=\"Serie\"):\n",
    "    \"\"\"\n",
    "    Pipeline SARIMAX con manejo correcto de zonas horarias\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ PIPELINE SARIMAX CORREGIDO - {nombre_serie}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "\n",
    "        \n",
    "        # 2. Crear variables ex√≥genas de forma segura\n",
    "        print(\"\\n2Ô∏è‚É£ Creando variables ex√≥genas...\")\n",
    "        evento_flag, serie_normalizada = crear_variable_exogena_segura(\n",
    "            serie, periodos_eventos, \"evento\"\n",
    "        )\n",
    "        \n",
    "        # 3. Verificar que tenemos datos suficientes\n",
    "        if evento_flag.sum() == 0:\n",
    "            print(\"‚ö†Ô∏è No se encontraron eventos en el rango de datos\")\n",
    "            print(\"Continuando con an√°lisis ARIMA simple...\")\n",
    "            \n",
    "            # Entrenar ARIMA simple como fallback\n",
    "            from statsmodels.tsa.arima.model import ARIMA\n",
    "            modelo_simple = ARIMA(serie_normalizada, order=(1,1,1)).fit()\n",
    "            \n",
    "            return modelo_simple, None, None\n",
    "        \n",
    "        # 4. Entrenar SARIMAX\n",
    "        print(f\"\\n3Ô∏è‚É£ Entrenando modelo SARIMAX...\")\n",
    "        print(f\"Variables ex√≥genas: {evento_flag.sum()} eventos de {len(evento_flag)} total\")\n",
    "        \n",
    "        # Usar SARIMAX sin componente estacional para evitar problemas\n",
    "        from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "        \n",
    "        modelo = SARIMAX(\n",
    "            endog=serie_normalizada,\n",
    "            exog=evento_flag.values.reshape(-1, 1),\n",
    "            order=(10, 0, 10),\n",
    "            seasonal_order=(0, 0, 0, 0),  # Sin estacionalidad\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        \n",
    "        modelo_ajustado = modelo.fit(disp=False)\n",
    "        \n",
    "        print(\"‚úÖ Modelo SARIMAX entrenado exitosamente\")\n",
    "        print(f\"AIC: {modelo_ajustado.aic:.2f}\")\n",
    "        \n",
    "        # 5. Mostrar resumen\n",
    "        print(\"\\n4Ô∏è‚É£ Resumen del modelo:\")\n",
    "        print(modelo_ajustado.summary())\n",
    "        \n",
    "        # 6. Visualizar ajuste\n",
    "        print(\"\\n5Ô∏è‚É£ Visualizando resultados...\")\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Subplot 1: Serie original vs ajustada\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(serie_normalizada, label='Serie Original', alpha=0.7)\n",
    "        plt.plot(modelo_ajustado.fittedvalues, label='Valores Ajustados', alpha=0.8)\n",
    "        plt.title('Serie Original vs Modelo Ajustado')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 2: Variable ex√≥gena\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(evento_flag, color='red', linewidth=2, label='Eventos')\n",
    "        plt.fill_between(evento_flag.index, 0, evento_flag, alpha=0.3, color='red')\n",
    "        plt.title('Variable Ex√≥gena - Eventos Importantes')\n",
    "        plt.ylabel('Flag (0/1)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 3: Residuos\n",
    "        plt.subplot(3, 1, 3)\n",
    "        residuos = modelo_ajustado.resid\n",
    "        plt.plot(residuos, color='orange', alpha=0.7)\n",
    "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        plt.title('Residuos del Modelo')\n",
    "        plt.xlabel('Tiempo')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 7. An√°lisis de coeficientes\n",
    "        print(\"\\n6Ô∏è‚É£ An√°lisis de coeficientes:\")\n",
    "        params = modelo_ajustado.params\n",
    "        pvalues = modelo_ajustado.pvalues\n",
    "        \n",
    "        for param_name in params.index:\n",
    "            if 'exog' in param_name or param_name in ['x1', 'exog']:\n",
    "                coef = params[param_name]\n",
    "                p_val = pvalues[param_name]\n",
    "                significativo = \"‚úÖ Significativo\" if p_val < 0.05 else \"‚ùå No significativo\"\n",
    "                print(f\"Variable ex√≥gena: {coef:.4f} (p-value: {p_val:.4f}) {significativo}\")\n",
    "        \n",
    "        return modelo_ajustado, evento_flag, serie_normalizada\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en el pipeline: {e}\")\n",
    "        print(\"Intentando con modelo ARIMA simple...\")\n",
    "        \n",
    "        # Fallback a ARIMA simple\n",
    "        try:\n",
    "            serie_limpia = serie.copy()\n",
    "            if hasattr(serie_limpia.index, 'tz') and serie_limpia.index.tz is not None:\n",
    "                serie_limpia.index = serie_limpia.index.tz_localize(None)\n",
    "            \n",
    "            from statsmodels.tsa.arima.model import ARIMA\n",
    "            modelo_fallback = ARIMA(serie_limpia, order=(1,1,1)).fit()\n",
    "            return modelo_fallback, None, serie_limpia\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Error tambi√©n en ARIMA: {e2}\")\n",
    "            return None, None, None\n",
    "\n",
    "# Ejecutar pipeline corregido\n",
    "resultado = pipeline_sarimax_corregido(train[\"2021-01-01\":], periodos, \"Precio SPY500\")\n",
    "modelo_final, eventos_final, serie_final = resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78752d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hacer_predicciones_sarimax(modelo, test, periodos_eventos):\n",
    "    \"\"\"\n",
    "    Hace predicciones con el modelo entrenado\n",
    "    \"\"\"\n",
    "    \n",
    "    num_periodos = len(test)\n",
    "\n",
    "    exogenas, serie_normalizada = crear_variable_exogena_segura(\n",
    "            test, periodos_eventos, \"evento\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "    predicciones = modelo.forecast(steps=num_periodos, exog=exogenas)\n",
    "    conf_int = modelo.get_forecast(steps=num_periodos, exog=exogenas).conf_int()\n",
    "    \n",
    "    return predicciones, conf_int\n",
    "\n",
    "predicciones, conf_int = hacer_predicciones_sarimax(modelo_final, test, periodos)\n",
    "\n",
    "#forecast = modelo_final.forecast(steps=426)\n",
    "#conf_int = modelo_final.get_forecast(steps=426).conf_int()\n",
    "\n",
    "#print(  len(forecast))\n",
    "#print(len(test))\n",
    "\n",
    "\n",
    "df_forecast = pd.DataFrame(predicciones)\n",
    "df_forecast.index = test.index\n",
    "\n",
    "\n",
    "sns.lineplot(df_forecast.loc[:\"2024-02-28\"])\n",
    "sns.lineplot(test.loc[:\"2024-02-28\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65274264",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install prophet scikit-learn plotly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Primero instalar Prophet si no lo tienes:\n",
    "# pip install prophet\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "def preparar_datos_prophet(serie, variables_exogenas=None):\n",
    "    \"\"\"\n",
    "    Prepara los datos en el formato requerido por Prophet\n",
    "    Prophet requiere columnas 'ds' (fechas) y 'y' (valores)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Preparando datos para Prophet...\")\n",
    "    \n",
    "    # Crear DataFrame base para Prophet\n",
    "    df_prophet = pd.DataFrame({\n",
    "        'ds': serie.index,\n",
    "        'y': serie.values\n",
    "    })\n",
    "    \n",
    "    # Agregar variables ex√≥genas si existen\n",
    "    if variables_exogenas is not None:\n",
    "        print(f\"üìä Agregando {variables_exogenas.shape[1]} variables ex√≥genas...\")\n",
    "        \n",
    "        # Alinear √≠ndices\n",
    "        fechas_comunes = serie.index.intersection(variables_exogenas.index)\n",
    "        df_prophet = df_prophet[df_prophet['ds'].isin(fechas_comunes)]\n",
    "        variables_alineadas = variables_exogenas.loc[fechas_comunes]\n",
    "        \n",
    "        # Agregar cada variable ex√≥gena\n",
    "        for col in variables_alineadas.columns:\n",
    "            df_prophet[col] = variables_alineadas[col].values\n",
    "            print(f\"  ‚úÖ Variable agregada: {col}\")\n",
    "    \n",
    "    # Asegurar que 'ds' sea datetime sin zona horaria\n",
    "    df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "    if df_prophet['ds'].dt.tz is not None:\n",
    "        df_prophet['ds'] = df_prophet['ds'].dt.tz_localize(None)\n",
    "    \n",
    "    print(f\"‚úÖ Datos preparados: {len(df_prophet)} observaciones\")\n",
    "    print(f\"Columnas: {list(df_prophet.columns)}\")\n",
    "    \n",
    "    return df_prophet\n",
    "\n",
    "\n",
    "\n",
    "def entrenar_prophet(df_train, variables_exogenas_nombres=None):\n",
    "    \"\"\"\n",
    "    Entrena modelo Prophet con variables ex√≥genas\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Entrenando modelo Prophet...\")\n",
    "    \n",
    "    # Crear modelo Prophet\n",
    "    modelo = Prophet(\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        changepoint_prior_scale=10,  # Flexibilidad para cambios de tendencia\n",
    "        seasonality_prior_scale=5,    # Flexibilidad para estacionalidad\n",
    "        holidays_prior_scale=10,       # Flexibilidad para efectos de holidays\n",
    "        interval_width=0.95           # Intervalo de confianza 95%\n",
    "    )\n",
    "    \n",
    "    # Agregar variables ex√≥genas como regressors\n",
    "    if variables_exogenas_nombres is not None:\n",
    "        for var_name in variables_exogenas_nombres:\n",
    "            if var_name in df_train.columns:\n",
    "                modelo.add_regressor(var_name)\n",
    "                print(f\"  ‚úÖ Regressor agregado: {var_name}\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    modelo.fit(df_train)\n",
    "    \n",
    "    print(\"‚úÖ Modelo Prophet entrenado exitosamente\")\n",
    "    return modelo\n",
    "\n",
    "def hacer_predicciones_prophet(modelo, df_test):\n",
    "    \"\"\"\n",
    "    Hace predicciones con Prophet\n",
    "    \"\"\"\n",
    "    print(f\"üîÆ Generando predicciones Prophet para {len(df_test)} per√≠odos...\")\n",
    "    \n",
    "    # Crear DataFrame futuro con las mismas columnas que el entrenamiento\n",
    "    future = df_test[['ds']].copy()\n",
    "    \n",
    "    # Agregar variables ex√≥genas para el futuro\n",
    "    for col in df_test.columns:\n",
    "        if col not in ['ds', 'y']:  # Excluir ds e y\n",
    "            future[col] = df_test[col].values\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    forecast = modelo.predict(future)\n",
    "    \n",
    "    print(\"‚úÖ Predicciones generadas\")\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc93ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodos_eventos = [\n",
    "    ['2022-02-01', \"2022-04-01\"],\n",
    "    [\"2023-03-01\", \"2023-05-01\"],\n",
    "    [\"2024-02-01\", \"2024-04-01\"],\n",
    "]\n",
    "\n",
    "# Usar tu funci√≥n crear_variable_exogena_segura\n",
    "evento_flag, ts_limpio = crear_variable_exogena_segura(ts, periodos_eventos, \"evento_importante\")\n",
    "\n",
    "df_prophet = preparar_datos_prophet(ts_limpio, evento_flag.to_frame())\n",
    "train = df_prophet.loc[df_prophet[\"ds\"]<=\"2023-12-31\"]\n",
    "test = df_prophet.loc[df_prophet[\"ds\"]>\"2023-12-31\"]\n",
    "\n",
    "modelo_prophet = entrenar_prophet(train, [\"evento_importante_flag\"])\n",
    "\n",
    "predicciones = hacer_predicciones_prophet(modelo_prophet, test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = hacer_predicciones_prophet(modelo_prophet, train)\n",
    "predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = predicciones, x = \"ds\", y = \"yhat\")\n",
    "sns.lineplot(data = train, x = \"ds\", y = \"y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a8de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def crear_variables_lag(serie, lags=None, prefijo='lag'):\n",
    "    \"\"\"\n",
    "    Crea variables lag de forma din√°mica evitando data leakage\n",
    "    \n",
    "    Parameters:\n",
    "    serie: Serie de tiempo\n",
    "    lags: Lista de lags a crear o entero para crear lags 1 hasta n\n",
    "    prefijo: Prefijo para nombrar las columnas\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame con variables lag\n",
    "    \"\"\"\n",
    "    if lags is None:\n",
    "        lags = [1, 2, 3, 5, 10]  # Default lags\n",
    "    elif isinstance(lags, int):\n",
    "        lags = list(range(1, lags + 1))  # Crear lags de 1 hasta n\n",
    "    \n",
    "    print(f\"üîÑ Creando variables lag: {lags}\")\n",
    "    \n",
    "    # Crear DataFrame resultado\n",
    "    df_lags = pd.DataFrame(index=serie.index)\n",
    "    \n",
    "    for lag in lags:\n",
    "        # Importante: shift(lag) usa valores PASADOS (evita data leakage)\n",
    "        df_lags[f'{prefijo}_{lag}'] = serie.shift(lag)\n",
    "        print(f\"  ‚úÖ {prefijo}_{lag} creado (datos {lag} per√≠odos atr√°s)\")\n",
    "    \n",
    "    # Mostrar informaci√≥n sobre valores nulos\n",
    "    valores_nulos = df_lags.isnull().sum()\n",
    "    print(f\"\\nüìä Valores nulos por variable:\")\n",
    "    for col, nulos in valores_nulos.items():\n",
    "        print(f\"  {col}: {nulos} valores nulos (normal para los primeros {col.split('_')[1]} registros)\")\n",
    "    \n",
    "    return df_lags\n",
    "\n",
    "def crear_promedios_moviles(serie, ventanas=None, prefijo='ma'):\n",
    "    \"\"\"\n",
    "    Crea promedios m√≥viles de n per√≠odos anteriores (evita data leakage)\n",
    "    \n",
    "    Parameters:\n",
    "    serie: Serie de tiempo\n",
    "    ventanas: Lista de ventanas o entero para ventanas 2 hasta n\n",
    "    prefijo: Prefijo para nombrar las columnas\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame con promedios m√≥viles\n",
    "    \"\"\"\n",
    "    if ventanas is None:\n",
    "        ventanas = [3, 5, 10, 20, 50]  # Default ventanas\n",
    "    elif isinstance(ventanas, int):\n",
    "        ventanas = list(range(2, ventanas + 1))  # Crear ventanas de 2 hasta n\n",
    "    \n",
    "    print(f\"üìä Creando promedios m√≥viles: {ventanas}\")\n",
    "    \n",
    "    # Crear DataFrame resultado\n",
    "    df_ma = pd.DataFrame(index=serie.index)\n",
    "    \n",
    "    for ventana in ventanas:\n",
    "        # CR√çTICO: usar shift(1) para evitar data leakage\n",
    "        # Calcula promedio de los √∫ltimos n per√≠odos ANTERIORES al actual\n",
    "        df_ma[f'{prefijo}_{ventana}'] = serie.shift(1).rolling(\n",
    "            window=ventana, \n",
    "            min_periods=ventana\n",
    "        ).mean()\n",
    "        \n",
    "        print(f\"  ‚úÖ {prefijo}_{ventana} creado (promedio de {ventana} per√≠odos anteriores)\")\n",
    "    \n",
    "    # Informaci√≥n sobre valores nulos\n",
    "    valores_nulos = df_ma.isnull().sum()\n",
    "    print(f\"\\nüìä Valores nulos por variable:\")\n",
    "    for col, nulos in valores_nulos.items():\n",
    "        ventana_size = int(col.split('_')[1])\n",
    "        print(f\"  {col}: {nulos} valores nulos (normal para los primeros {ventana_size} registros)\")\n",
    "    \n",
    "    return df_ma\n",
    "\n",
    "def crear_std_moviles(serie, ventanas=None, prefijo='std'):\n",
    "    \"\"\"\n",
    "    Crea desviaciones est√°ndar m√≥viles de n per√≠odos anteriores (evita data leakage)\n",
    "    \n",
    "    Parameters:\n",
    "    serie: Serie de tiempo\n",
    "    ventanas: Lista de ventanas o entero para ventanas 2 hasta n\n",
    "    prefijo: Prefijo para nombrar las columnas\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame con desviaciones est√°ndar m√≥viles\n",
    "    \"\"\"\n",
    "    if ventanas is None:\n",
    "        ventanas = [3, 5, 10, 20, 50]  # Default ventanas\n",
    "    elif isinstance(ventanas, int):\n",
    "        ventanas = list(range(2, ventanas + 1))  # Crear ventanas de 2 hasta n\n",
    "    \n",
    "    print(f\"üìà Creando desviaciones est√°ndar m√≥viles: {ventanas}\")\n",
    "    \n",
    "    # Crear DataFrame resultado\n",
    "    df_std = pd.DataFrame(index=serie.index)\n",
    "    \n",
    "    for ventana in ventanas:\n",
    "        # CR√çTICO: usar shift(1) para evitar data leakage\n",
    "        # Calcula std de los √∫ltimos n per√≠odos ANTERIORES al actual\n",
    "        df_std[f'{prefijo}_{ventana}'] = serie.shift(1).rolling(\n",
    "            window=ventana, \n",
    "            min_periods=ventana\n",
    "        ).std()\n",
    "        \n",
    "        print(f\"  ‚úÖ {prefijo}_{ventana} creado (std de {ventana} per√≠odos anteriores)\")\n",
    "    \n",
    "    # Informaci√≥n sobre valores nulos\n",
    "    valores_nulos = df_std.isnull().sum()\n",
    "    print(f\"\\nüìä Valores nulos por variable:\")\n",
    "    for col, nulos in valores_nulos.items():\n",
    "        ventana_size = int(col.split('_')[1])\n",
    "        print(f\"  {col}: {nulos} valores nulos (normal para los primeros {ventana_size} registros)\")\n",
    "    \n",
    "    return df_std\n",
    "\n",
    "def crear_variables_rolling_avanzadas(serie, ventanas=None, prefijo='roll'):\n",
    "    \"\"\"\n",
    "    Crea variables rolling avanzadas (min, max, mediana) evitando data leakage\n",
    "    \"\"\"\n",
    "    if ventanas is None:\n",
    "        ventanas = [5, 10, 20]\n",
    "    elif isinstance(ventanas, int):\n",
    "        ventanas = list(range(2, ventanas + 1))\n",
    "    \n",
    "    print(f\"üîß Creando variables rolling avanzadas: {ventanas}\")\n",
    "    \n",
    "    df_roll = pd.DataFrame(index=serie.index)\n",
    "    \n",
    "    for ventana in ventanas:\n",
    "        # Todos usan shift(1) para evitar data leakage\n",
    "        rolling_data = serie.shift(1).rolling(window=ventana, min_periods=ventana)\n",
    "        \n",
    "        df_roll[f'{prefijo}_min_{ventana}'] = rolling_data.min()\n",
    "        df_roll[f'{prefijo}_max_{ventana}'] = rolling_data.max()\n",
    "        df_roll[f'{prefijo}_median_{ventana}'] = rolling_data.median()\n",
    "        df_roll[f'{prefijo}_q25_{ventana}'] = rolling_data.quantile(0.25)\n",
    "        df_roll[f'{prefijo}_q75_{ventana}'] = rolling_data.quantile(0.75)\n",
    "        \n",
    "        print(f\"  ‚úÖ Variables rolling {ventana} creadas (min, max, median, q25, q75)\")\n",
    "    \n",
    "    return df_roll\n",
    "\n",
    "def crear_variables_tecnicas(serie, ventanas_cortas=None, ventanas_largas=None):\n",
    "    \"\"\"\n",
    "    Crea variables t√©cnicas comunes evitando data leakage\n",
    "    \"\"\"\n",
    "    if ventanas_cortas is None:\n",
    "        ventanas_cortas = [5, 10, 20]\n",
    "    if ventanas_largas is None:\n",
    "        ventanas_largas = [50, 100, 200]\n",
    "    \n",
    "    print(f\"üìà Creando variables t√©cnicas...\")\n",
    "    \n",
    "    df_tech = pd.DataFrame(index=serie.index)\n",
    "    \n",
    "    # RSI (Relative Strength Index) - usando datos pasados\n",
    "    for ventana in [14, 30]:\n",
    "        delta = serie.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).shift(1).rolling(window=ventana).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).shift(1).rolling(window=ventana).mean()\n",
    "        rs = gain / loss\n",
    "        df_tech[f'rsi_{ventana}'] = 100 - (100 / (1 + rs))\n",
    "        print(f\"  ‚úÖ RSI_{ventana} creado\")\n",
    "    \n",
    "    # Bandas de Bollinger\n",
    "    for ventana in ventanas_cortas:\n",
    "        ma = serie.shift(1).rolling(window=ventana).mean()\n",
    "        std = serie.shift(1).rolling(window=ventana).std()\n",
    "        df_tech[f'bb_upper_{ventana}'] = ma + (2 * std)\n",
    "        df_tech[f'bb_lower_{ventana}'] = ma - (2 * std)\n",
    "        df_tech[f'bb_width_{ventana}'] = df_tech[f'bb_upper_{ventana}'] - df_tech[f'bb_lower_{ventana}']\n",
    "        print(f\"  ‚úÖ Bandas Bollinger {ventana} creadas\")\n",
    "    \n",
    "    # MACD (Moving Average Convergence Divergence)\n",
    "    ema12 = serie.shift(1).ewm(span=12).mean()\n",
    "    ema26 = serie.shift(1).ewm(span=26).mean()\n",
    "    df_tech['macd'] = ema12 - ema26\n",
    "    df_tech['macd_signal'] = df_tech['macd'].shift(1).ewm(span=9).mean()\n",
    "    df_tech['macd_histogram'] = df_tech['macd'] - df_tech['macd_signal']\n",
    "    print(f\"  ‚úÖ MACD creado\")\n",
    "    \n",
    "    return df_tech\n",
    "\n",
    "def crear_features_completas(serie, config=None):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal que crea todas las features evitando data leakage\n",
    "    \n",
    "    Parameters:\n",
    "    serie: Serie de tiempo\n",
    "    config: Diccionario con configuraci√≥n personalizada\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'lags': [1, 2, 3, 5, 10, 20],\n",
    "            'ventanas_ma': [3, 5, 10, 20, 50],\n",
    "            'ventanas_std': [5, 10, 20],\n",
    "            'ventanas_rolling': [5, 10, 20],\n",
    "            'incluir_tecnicas': True\n",
    "        }\n",
    "    \n",
    "    print(\"üöÄ CREANDO FEATURES COMPLETAS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Normalizar zona horaria\n",
    "    if hasattr(serie.index, 'tz') and serie.index.tz is not None:\n",
    "        serie = serie.copy()\n",
    "        serie.index = serie.index.tz_localize(None)\n",
    "        print(\"‚úÖ Zona horaria normalizada\")\n",
    "    \n",
    "    # Crear todas las variables\n",
    "    features_completas = pd.DataFrame(index=serie.index)\n",
    "    \n",
    "    # 1. Variables lag\n",
    "    print(\"\\n1Ô∏è‚É£ Creando variables lag...\")\n",
    "    df_lags = crear_variables_lag(serie, config['lags'])\n",
    "    features_completas = pd.concat([features_completas, df_lags], axis=1)\n",
    "    \n",
    "    # 2. Promedios m√≥viles\n",
    "    print(\"\\n2Ô∏è‚É£ Creando promedios m√≥viles...\")\n",
    "    df_ma = crear_promedios_moviles(serie, config['ventanas_ma'])\n",
    "    features_completas = pd.concat([features_completas, df_ma], axis=1)\n",
    "    \n",
    "    # 3. Desviaciones est√°ndar m√≥viles\n",
    "    print(\"\\n3Ô∏è‚É£ Creando desviaciones est√°ndar m√≥viles...\")\n",
    "    df_std = crear_std_moviles(serie, config['ventanas_std'])\n",
    "    features_completas = pd.concat([features_completas, df_std], axis=1)\n",
    "    \n",
    "    # 4. Variables rolling avanzadas\n",
    "    print(\"\\n4Ô∏è‚É£ Creando variables rolling avanzadas...\")\n",
    "    df_roll = crear_variables_rolling_avanzadas(serie, config['ventanas_rolling'])\n",
    "    features_completas = pd.concat([features_completas, df_roll], axis=1)\n",
    "    \n",
    "    # 5. Variables t√©cnicas\n",
    "    if config['incluir_tecnicas']:\n",
    "        print(\"\\n5Ô∏è‚É£ Creando variables t√©cnicas...\")\n",
    "        df_tech = crear_variables_tecnicas(serie)\n",
    "        features_completas = pd.concat([features_completas, df_tech], axis=1)\n",
    "    \n",
    "    # 6. Variable objetivo (valor actual)\n",
    "    features_completas['target'] = serie\n",
    "    \n",
    "    # 7. Resumen final\n",
    "    print(f\"\\nüìä RESUMEN FINAL:\")\n",
    "    print(f\"  Total features creadas: {len(features_completas.columns) - 1}\")  # -1 por target\n",
    "    print(f\"  Total observaciones: {len(features_completas)}\")\n",
    "    print(f\"  Rango de fechas: {features_completas.index.min()} a {features_completas.index.max()}\")\n",
    "    \n",
    "    # Informaci√≥n sobre valores nulos\n",
    "    valores_nulos = features_completas.isnull().sum()\n",
    "    features_con_nulos = valores_nulos[valores_nulos > 0]\n",
    "    \n",
    "    if len(features_con_nulos) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Variables con valores nulos:\")\n",
    "        for var, nulos in features_con_nulos.items():\n",
    "            print(f\"  {var}: {nulos} valores nulos\")\n",
    "        \n",
    "        # Eliminar filas con muchos nulos (primeras filas t√≠picamente)\n",
    "        max_lag = max(config['lags']) if config['lags'] else 0\n",
    "        max_ventana = max(config['ventanas_ma'] + config['ventanas_std'] + config['ventanas_rolling'])\n",
    "        min_observaciones_validas = max(max_lag, max_ventana)\n",
    "        \n",
    "        print(f\"\\nüßπ Recomendaci√≥n: Eliminar las primeras {min_observaciones_validas} filas\")\n",
    "        features_limpias = features_completas.iloc[min_observaciones_validas:].copy()\n",
    "        print(f\"‚úÖ Dataset limpio: {len(features_limpias)} observaciones\")\n",
    "        \n",
    "        return features_limpias\n",
    "    \n",
    "    return features_completas\n",
    "\n",
    "def verificar_no_data_leakage(features, target_col='target', mostrar_ejemplo=True):\n",
    "    \"\"\"\n",
    "    Verifica que no hay data leakage en las features creadas\n",
    "    \"\"\"\n",
    "    print(\"üîç VERIFICACI√ìN DE DATA LEAKAGE\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Seleccionar una fecha random para verificar\n",
    "    if mostrar_ejemplo and len(features) > 100:\n",
    "        idx_random = np.random.randint(50, len(features) - 10)\n",
    "        fecha_ejemplo = features.index[idx_random]\n",
    "        \n",
    "        print(f\"üìÖ Verificando fecha ejemplo: {fecha_ejemplo}\")\n",
    "        print(f\"Valor target en esa fecha: {features.loc[fecha_ejemplo, target_col]:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüîç Verificando que las features usan solo datos ANTERIORES:\")\n",
    "        \n",
    "        # Verificar algunas variables lag\n",
    "        lag_cols = [col for col in features.columns if 'lag_' in col][:3]\n",
    "        for col in lag_cols:\n",
    "            lag_num = int(col.split('_')[1])\n",
    "            fecha_anterior = features.index[idx_random - lag_num]\n",
    "            valor_lag = features.loc[fecha_ejemplo, col]\n",
    "            valor_real_anterior = features.loc[fecha_anterior, target_col]\n",
    "            \n",
    "            print(f\"  {col}: {valor_lag:.4f} == valor real {lag_num} per√≠odos antes: {valor_real_anterior:.4f}\")\n",
    "            assert abs(valor_lag - valor_real_anterior) < 1e-6, f\"Data leakage en {col}!\"\n",
    "        \n",
    "        # Verificar promedio m√≥vil\n",
    "        ma_cols = [col for col in features.columns if 'ma_' in col][:2]\n",
    "        for col in ma_cols:\n",
    "            ventana = int(col.split('_')[1])\n",
    "            fecha_inicio = idx_random - ventana\n",
    "            fecha_fin = idx_random - 1  # No incluir el d√≠a actual\n",
    "            \n",
    "            valores_anteriores = features.loc[features.index[fecha_inicio:fecha_fin+1], target_col]\n",
    "            ma_calculado = valores_anteriores.mean()\n",
    "            ma_feature = features.loc[fecha_ejemplo, col]\n",
    "            \n",
    "            print(f\"  {col}: {ma_feature:.4f} == promedio calculado: {ma_calculado:.4f}\")\n",
    "            assert abs(ma_feature - ma_calculado) < 1e-4, f\"Data leakage en {col}!\"\n",
    "        \n",
    "        print(\"‚úÖ No se detect√≥ data leakage - Las features usan solo datos pasados\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No hay suficientes datos para verificaci√≥n completa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de72919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haciendolo en modelos de machine learning\n",
    "# Creando nuestro dataset\n",
    "\n",
    "# Ejemplo con tu dataset\n",
    "# Supongamos que tu serie se llama 'ts'\n",
    "\n",
    "# Configuraci√≥n personalizada\n",
    "config_personalizada = {\n",
    "    'lags': [1, 2, 3, 4, 5, 6,7,8,9, 10,11,12,13,14,15,16,17,18,19,20, 50,60,90,120,180,240,360],  # Lags que quieres\n",
    "    'ventanas_ma': [5, 10, 20, 50, 100],  # Ventanas para promedios m√≥viles\n",
    "    'ventanas_std': [5, 10, 20, 50],  # Ventanas para std m√≥vil\n",
    "    'ventanas_rolling': [10, 20, 50],  # Ventanas para min/max/median\n",
    "    'incluir_tecnicas': True  # Incluir RSI, MACD, Bollinger\n",
    "}\n",
    "\n",
    "# Crear todas las features\n",
    "features_dataset = crear_features_completas(ts, config_personalizada)\n",
    "\n",
    "# Verificar que no hay data leakage\n",
    "verificar_no_data_leakage(features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def crear_variables_fechas_basicas(serie):\n",
    "    \"\"\"\n",
    "    Crea variables b√°sicas de fechas\n",
    "    \"\"\"\n",
    "    print(\"üìÖ Creando variables b√°sicas de fechas...\")\n",
    "    \n",
    "    # Normalizar zona horaria\n",
    "    if hasattr(serie.index, 'tz') and serie.index.tz is not None:\n",
    "        serie = serie.copy()\n",
    "        serie.index = serie.index.tz_localize(None)\n",
    "    \n",
    "    #df_fechas = pd.DataFrame(index=serie.index)\n",
    "    df_fechas = serie.copy()\n",
    "    \n",
    "    # Variables b√°sicas\n",
    "    df_fechas['a√±o'] = serie.index.year\n",
    "    df_fechas['mes'] = serie.index.month\n",
    "    df_fechas['dia'] = serie.index.day\n",
    "    df_fechas['dia_semana'] = serie.index.dayofweek  # 0=Lunes, 6=Domingo\n",
    "    df_fechas['dia_a√±o'] = serie.index.dayofyear\n",
    "    df_fechas['semana_a√±o'] = serie.index.isocalendar().week\n",
    "    df_fechas['trimestre'] = serie.index.quarter\n",
    "    \n",
    "    # Variables categ√≥ricas √∫tiles\n",
    "    df_fechas['es_fin_semana'] = (serie.index.dayofweek >= 5).astype(int)\n",
    "    df_fechas['es_lunes'] = (serie.index.dayofweek == 0).astype(int)\n",
    "    df_fechas['es_viernes'] = (serie.index.dayofweek == 4).astype(int)\n",
    "    \n",
    "    # Variables c√≠clicas (√∫tiles para ML)\n",
    "    df_fechas['mes_sin'] = np.sin(2 * np.pi * serie.index.month / 12)\n",
    "    df_fechas['mes_cos'] = np.cos(2 * np.pi * serie.index.month / 12)\n",
    "    df_fechas['dia_semana_sin'] = np.sin(2 * np.pi * serie.index.dayofweek / 7)\n",
    "    df_fechas['dia_semana_cos'] = np.cos(2 * np.pi * serie.index.dayofweek / 7)\n",
    "    \n",
    "    print(f\"‚úÖ Variables b√°sicas creadas: {len(df_fechas.columns)} variables\")\n",
    "    return df_fechas\n",
    "\n",
    "def calcular_distancia_navidad(fecha):\n",
    "    \"\"\"\n",
    "    Calcula las semanas que faltan para la pr√≥xima Navidad\n",
    "    \"\"\"\n",
    "    a√±o_actual = fecha.year\n",
    "    navidad_este_a√±o = datetime(a√±o_actual, 12, 25)\n",
    "    \n",
    "    # Si ya pas√≥ Navidad este a√±o, calcular para el pr√≥ximo\n",
    "    if fecha > navidad_este_a√±o:\n",
    "        navidad_proxima = datetime(a√±o_actual + 1, 12, 25)\n",
    "    else:\n",
    "        navidad_proxima = navidad_este_a√±o\n",
    "    \n",
    "    dias_diferencia = (navidad_proxima - fecha).days\n",
    "    semanas_diferencia = dias_diferencia / 7\n",
    "    \n",
    "    return semanas_diferencia\n",
    "\n",
    "def calcular_distancia_a√±o_nuevo(fecha):\n",
    "    \"\"\"\n",
    "    Calcula las semanas que faltan para el pr√≥ximo A√±o Nuevo\n",
    "    \"\"\"\n",
    "    a√±o_siguiente = fecha.year + 1\n",
    "    a√±o_nuevo = datetime(a√±o_siguiente, 1, 1)\n",
    "    \n",
    "    dias_diferencia = (a√±o_nuevo - fecha).days\n",
    "    semanas_diferencia = dias_diferencia / 7\n",
    "    \n",
    "    return semanas_diferencia\n",
    "\n",
    "def crear_variables_fechas_especiales(serie):\n",
    "    \"\"\"\n",
    "    Crea variables relacionadas con fechas especiales y festividades\n",
    "    \"\"\"\n",
    "    print(\"üéÑ Creando variables de fechas especiales...\")\n",
    "    \n",
    "    # Normalizar zona horaria\n",
    "    if hasattr(serie.index, 'tz') and serie.index.tz is not None:\n",
    "        serie = serie.copy()\n",
    "        serie.index = serie.index.tz_localize(None)\n",
    "    \n",
    "    df_especiales = serie.copy()\n",
    "    \n",
    "    # Distancias a festividades importantes\n",
    "    df_especiales['semanas_hasta_navidad'] = [\n",
    "        calcular_distancia_navidad(fecha) for fecha in serie.index\n",
    "    ]\n",
    "    \n",
    "    df_especiales['semanas_hasta_a√±o_nuevo'] = [\n",
    "        calcular_distancia_a√±o_nuevo(fecha) for fecha in serie.index\n",
    "    ]\n",
    "    \n",
    "    # Per√≠odos especiales del a√±o\n",
    "    df_especiales['es_diciembre'] = (serie.index.month == 12).astype(int)\n",
    "    df_especiales['es_enero'] = (serie.index.month == 1).astype(int)\n",
    "    df_especiales['es_q4'] = (serie.index.quarter == 4).astype(int)\n",
    "    df_especiales['es_q1'] = (serie.index.quarter == 1).astype(int)\n",
    "    \n",
    "    # Temporada navide√±a (noviembre-diciembre)\n",
    "    df_especiales['temporada_navidena'] = (\n",
    "        (serie.index.month == 11) | (serie.index.month == 12)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Black Friday (4to viernes de noviembre - aproximaci√≥n)\n",
    "    df_especiales['cerca_black_friday'] = 0\n",
    "    for i, fecha in enumerate(serie.index):\n",
    "        if fecha.month == 11:\n",
    "            # Encontrar el 4to viernes de noviembre\n",
    "            primer_dia = datetime(fecha.year, 11, 1)\n",
    "            dias_hasta_viernes = (4 - primer_dia.weekday()) % 7\n",
    "            primer_viernes = primer_dia + timedelta(days=dias_hasta_viernes)\n",
    "            cuarto_viernes = primer_viernes + timedelta(days=21)  # 3 semanas despu√©s\n",
    "            \n",
    "            # Marcar la semana del Black Friday\n",
    "            if abs((fecha - cuarto_viernes).days) <= 3:\n",
    "                df_especiales.iloc[i, df_especiales.columns.get_loc('cerca_black_friday')] = 1\n",
    "    \n",
    "    # Inicio/fin de a√±o fiscal (para acciones)\n",
    "    df_especiales['fin_a√±o_fiscal'] = (\n",
    "        (serie.index.month == 12) & (serie.index.day >= 25)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_especiales['inicio_a√±o_fiscal'] = (\n",
    "        (serie.index.month == 1) & (serie.index.day <= 10)\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"‚úÖ Variables especiales creadas: {len(df_especiales.columns)} variables\")\n",
    "    return df_especiales\n",
    "\n",
    "def crear_variables_estacionales(serie):\n",
    "    \"\"\"\n",
    "    Crea variables estacionales m√°s sofisticadas\n",
    "    \"\"\"\n",
    "    print(\"üå∏ Creando variables estacionales...\")\n",
    "    \n",
    "    # Normalizar zona horaria\n",
    "    if hasattr(serie.index, 'tz') and serie.index.tz is not None:\n",
    "        serie = serie.copy()\n",
    "        serie.index = serie.index.tz_localize(None)\n",
    "    \n",
    "    df_estacional = serie.copy()\n",
    "    \n",
    "    # Estaciones del a√±o (hemisferio norte)\n",
    "    def obtener_estacion(fecha):\n",
    "        mes = fecha.month\n",
    "        if mes in [12, 1, 2]:\n",
    "            return 'invierno'\n",
    "        elif mes in [3, 4, 5]:\n",
    "            return 'primavera'\n",
    "        elif mes in [6, 7, 8]:\n",
    "            return 'verano'\n",
    "        else:\n",
    "            return 'oto√±o'\n",
    "    \n",
    "    estaciones = [obtener_estacion(fecha) for fecha in serie.index]\n",
    "    \n",
    "    # One-hot encoding para estaciones\n",
    "    df_estacional['es_invierno'] = (pd.Series(estaciones, index=serie.index) == 'invierno').astype(int)\n",
    "    df_estacional['es_primavera'] = (pd.Series(estaciones, index=serie.index) == 'primavera').astype(int)\n",
    "    df_estacional['es_verano'] = (pd.Series(estaciones, index=serie.index) == 'verano').astype(int)\n",
    "    df_estacional['es_oto√±o'] = (pd.Series(estaciones, index=serie.index) == 'oto√±o').astype(int)\n",
    "    \n",
    "    # Variables de intensidad estacional (c√≠clicas)\n",
    "    dia_a√±o_normalizado = serie.index.dayofyear / 365.25\n",
    "    df_estacional['estacion_sin'] = np.sin(2 * np.pi * dia_a√±o_normalizado)\n",
    "    df_estacional['estacion_cos'] = np.cos(2 * np.pi * dia_a√±o_normalizado)\n",
    "    \n",
    "    # Patrones bimestrales y trimestrales\n",
    "    df_estacional['bimestre'] = ((serie.index.month - 1) // 2) + 1\n",
    "    df_estacional['bimestre_sin'] = np.sin(2 * np.pi * df_estacional['bimestre'] / 6)\n",
    "    df_estacional['bimestre_cos'] = np.cos(2 * np.pi * df_estacional['bimestre'] / 6)\n",
    "    \n",
    "    print(f\"‚úÖ Variables estacionales creadas: {len(df_estacional.columns)} variables\")\n",
    "    return df_estacional\n",
    "\n",
    "def crear_variables_mercado_financiero(serie):\n",
    "    \"\"\"\n",
    "    Crea variables espec√≠ficas para mercados financieros\n",
    "    \"\"\"\n",
    "    print(\"üìà Creando variables espec√≠ficas de mercados financieros...\")\n",
    "    \n",
    "    # Normalizar zona horaria\n",
    "    if hasattr(serie.index, 'tz') and serie.index.tz is not None:\n",
    "        serie = serie.copy()\n",
    "        serie.index = serie.index.tz_localize(None)\n",
    "    \n",
    "    df_mercado = serie.copy()\n",
    "    \n",
    "    # D√≠as de la semana espec√≠ficos para trading\n",
    "    df_mercado['es_lunes_trading'] = (serie.index.dayofweek == 0).astype(int)  # Efecto lunes\n",
    "    df_mercado['es_viernes_trading'] = (serie.index.dayofweek == 4).astype(int)  # Efecto viernes\n",
    "    \n",
    "    # Efectos de fin de mes/trimestre/a√±o\n",
    "    def es_ultimo_dia_mes(fecha):\n",
    "        siguiente_mes = fecha + timedelta(days=1)\n",
    "        return siguiente_mes.month != fecha.month\n",
    "    \n",
    "    def es_ultimo_dia_trimestre(fecha):\n",
    "        return fecha.month in [3, 6, 9, 12] and es_ultimo_dia_mes(fecha)\n",
    "    \n",
    "    df_mercado['fin_de_mes'] = [es_ultimo_dia_mes(fecha) for fecha in serie.index]\n",
    "    df_mercado['fin_de_trimestre'] = [es_ultimo_dia_trimestre(fecha) for fecha in serie.index]\n",
    "    \n",
    "    # Primeros d√≠as del mes (efecto rebalanceo)\n",
    "    df_mercado['primeros_5_dias_mes'] = (serie.index.day <= 5).astype(int)\n",
    "    df_mercado['ultimos_5_dias_mes'] = [\n",
    "        es_ultimo_dia_mes(fecha + timedelta(days=4)) for fecha in serie.index\n",
    "    ]\n",
    "    \n",
    "    # Per√≠odos de earnings (trimestres)\n",
    "    df_mercado['periodo_earnings'] = (\n",
    "        ((serie.index.month == 1) & (serie.index.day <= 31)) |  # Q4 earnings\n",
    "        ((serie.index.month == 4) & (serie.index.day <= 30)) |  # Q1 earnings\n",
    "        ((serie.index.month == 7) & (serie.index.day <= 31)) |  # Q2 earnings\n",
    "        ((serie.index.month == 10) & (serie.index.day <= 31))   # Q3 earnings\n",
    "    ).astype(int)\n",
    "    \n",
    "    # D√≠as festivos burs√°tiles importantes (aproximados)\n",
    "    def es_cerca_festivo_bursatil(fecha):\n",
    "        # A√±o nuevo, Memorial Day, Independence Day, Labor Day, Thanksgiving, Christmas\n",
    "        festivos_fijos = [\n",
    "            (1, 1),   # A√±o nuevo\n",
    "            (7, 4),   # Independence Day\n",
    "            (12, 25), # Christmas\n",
    "        ]\n",
    "        \n",
    "        for mes, dia in festivos_fijos:\n",
    "            festivo = datetime(fecha.year, mes, dia)\n",
    "            if abs((fecha - festivo).days) <= 1:  # 1 d√≠a antes/despu√©s\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    df_mercado['cerca_festivo_bursatil'] = [\n",
    "        es_cerca_festivo_bursatil(fecha) for fecha in serie.index\n",
    "    ]\n",
    "    \n",
    "    print(f\"‚úÖ Variables de mercado creadas: {len(df_mercado.columns)} variables\")\n",
    "    return df_mercado\n",
    "\n",
    "def crear_todas_variables_fechas(serie, incluir_mercado=True):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal que crea todas las variables de fechas\n",
    "    \"\"\"\n",
    "    print(\"üöÄ CREANDO TODAS LAS VARIABLES DE FECHAS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Normalizar zona horaria\n",
    "    if hasattr(serie.index, 'tz') and serie.index.tz is not None:\n",
    "        serie = serie.copy()\n",
    "        serie.index = serie.index.tz_localize(None)\n",
    "    \n",
    "    # Crear todas las variables\n",
    "    variables_fechas = serie.copy()\n",
    "    \n",
    "    # 1. Variables b√°sicas\n",
    "    print(\"\\n1Ô∏è‚É£ Variables b√°sicas...\")\n",
    "    df_basicas = crear_variables_fechas_basicas(serie)\n",
    "    variables_fechas = pd.concat([variables_fechas, df_basicas], axis=1)\n",
    "    \n",
    "    # 2. Variables especiales\n",
    "    print(\"\\n2Ô∏è‚É£ Variables especiales...\")\n",
    "    df_especiales = crear_variables_fechas_especiales(serie)\n",
    "    variables_fechas = pd.concat([variables_fechas, df_especiales], axis=1)\n",
    "    \n",
    "    # 3. Variables estacionales\n",
    "    print(\"\\n3Ô∏è‚É£ Variables estacionales...\")\n",
    "    df_estacionales = crear_variables_estacionales(serie)\n",
    "    variables_fechas = pd.concat([variables_fechas, df_estacionales], axis=1)\n",
    "    \n",
    "    # 4. Variables de mercado (opcional)\n",
    "    if incluir_mercado:\n",
    "        print(\"\\n4Ô∏è‚É£ Variables de mercado financiero...\")\n",
    "        df_mercado = crear_variables_mercado_financiero(serie)\n",
    "        variables_fechas = pd.concat([variables_fechas, df_mercado], axis=1)\n",
    "    \n",
    "    # Resumen final\n",
    "    print(f\"\\nüìä RESUMEN FINAL:\")\n",
    "    print(f\"  Total variables de fechas creadas: {len(variables_fechas.columns)}\")\n",
    "    print(f\"  Observaciones: {len(variables_fechas)}\")\n",
    "    \n",
    "    # Mostrar algunas variables de ejemplo\n",
    "    print(f\"\\nüìÖ Variables creadas:\")\n",
    "    for i, col in enumerate(variables_fechas.columns):\n",
    "        if i < 10:  # Mostrar primeras 10\n",
    "            print(f\"  ‚úÖ {col}\")\n",
    "        elif i == 10:\n",
    "            print(f\"  ... y {len(variables_fechas.columns) - 10} m√°s\")\n",
    "            break\n",
    "    \n",
    "    return variables_fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final = crear_todas_variables_fechas(features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final = dataset_final.T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar las predicciones de Prophet como variable\n",
    "\n",
    "periodos_eventos = [\n",
    "    ['2022-02-01', \"2022-04-01\"],\n",
    "    [\"2023-03-01\", \"2023-05-01\"],\n",
    "    [\"2024-02-01\", \"2024-04-01\"],\n",
    "]\n",
    "\n",
    "# Usar tu funci√≥n crear_variable_exogena_segura\n",
    "evento_flag, ts_limpio = crear_variable_exogena_segura(ts, periodos_eventos, \"evento_importante\")\n",
    "\n",
    "df_prophet = preparar_datos_prophet(ts_limpio, evento_flag.to_frame())\n",
    "train_prophet = df_prophet.loc[df_prophet[\"ds\"]<=\"2023-12-31\"]\n",
    "test_prophet = df_prophet.loc[df_prophet[\"ds\"]>\"2023-12-31\"]\n",
    "\n",
    "modelo_prophet = entrenar_prophet(train_prophet, [\"evento_importante_flag\"])\n",
    "\n",
    "predicciones_test = hacer_predicciones_prophet(modelo_prophet, test_prophet)\n",
    "predicciones_train = hacer_predicciones_prophet(modelo_prophet, train_prophet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prophet = pd.concat([predicciones_train, predicciones_test])\n",
    "df_prophet = df_prophet[[\"ds\",\"yhat\"]]\n",
    "df_prophet = df_prophet.rename(columns={\"ds\":\"Date\"})\n",
    "df_prophet[\"Date\"] = pd.to_datetime(df_prophet[\"Date\"].dt.date)\n",
    "dataset_final_no_index = dataset_final.reset_index()\n",
    "dataset_final_no_index = pd.merge(dataset_final_no_index, df_prophet, on=\"Date\", how=\"left\")\n",
    "dataset_final_no_index.set_index(\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_momentum_simple(serie, ventanas=None, prefijo='momentum'):\n",
    "    \"\"\"\n",
    "    Calcula momentum simple (cambio porcentual) para diferentes ventanas\n",
    "    \n",
    "    Parameters:\n",
    "    serie: Serie de tiempo\n",
    "    ventanas: Lista de ventanas temporales\n",
    "    prefijo: Prefijo para nombrar columnas\n",
    "    \"\"\"\n",
    "    if ventanas is None:\n",
    "        ventanas = [1, 2, 3, 5, 10, 20]\n",
    "    \n",
    "    print(f\"üìà Calculando momentum simple para ventanas: {ventanas}\")\n",
    "    serie_ = serie[\"target\"]\n",
    "    df_momentum = serie.copy()\n",
    "    #df_momentum = df_momentum[[\"target\"]]\n",
    "    for ventana in ventanas:\n",
    "        # Momentum = (valor_actual - valor_anterior) / valor_anterior\n",
    "        df_momentum[f'{prefijo}_{ventana}'] = serie_.pct_change(ventana)\n",
    "        \n",
    "        # Momentum absoluto (cambio en puntos)\n",
    "        df_momentum[f'{prefijo}_abs_{ventana}'] = serie_.diff(ventana)\n",
    "        \n",
    "        print(f\"  ‚úÖ Momentum {ventana} per√≠odos creado\")\n",
    "    \n",
    "    return df_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c289f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final_no_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be654e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final_no_index = calcular_momentum_simple(dataset_final_no_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def calcular_velocidad_momentum_corregida(serie, ventanas=None, metodo='lineal', columna_target='target'):\n",
    "    \"\"\"\n",
    "    Calcula velocidad usando regresi√≥n lineal sobre n per√≠odos anteriores\n",
    "    \n",
    "    Parameters:\n",
    "    serie: Serie de tiempo o DataFrame\n",
    "    ventanas: Lista de ventanas para calcular velocidad\n",
    "    metodo: 'lineal', 'polinomial', 'log'\n",
    "    columna_target: Nombre de la columna objetivo si serie es DataFrame\n",
    "    \"\"\"\n",
    "    if ventanas is None:\n",
    "        ventanas = [3, 5, 10, 20]\n",
    "    \n",
    "    print(f\"üöÄ Calculando velocidad momentum (m√©todo: {metodo}) para ventanas: {ventanas}\")\n",
    "    \n",
    "    # CORRECCI√ìN 1: Manejar tanto Series como DataFrames\n",
    "    if isinstance(serie, pd.DataFrame):\n",
    "        if columna_target not in serie.columns:\n",
    "            raise ValueError(f\"Columna '{columna_target}' no encontrada. Columnas disponibles: {list(serie.columns)}\")\n",
    "        serie_valores = serie[columna_target]\n",
    "        indice_original = serie.index\n",
    "    else:\n",
    "        serie_valores = serie\n",
    "        indice_original = serie.index\n",
    "    \n",
    "    print(f\"üìä Procesando serie con {len(serie_valores)} valores\")\n",
    "    \n",
    "    # CORRECCI√ìN 2: Crear DataFrame resultado con el √≠ndice correcto\n",
    "    df_velocidad = pd.DataFrame(index=indice_original)\n",
    "    \n",
    "    for ventana in ventanas:\n",
    "        print(f\"\\nüîÑ Procesando ventana {ventana}...\")\n",
    "        velocidades = []\n",
    "        \n",
    "        for i in range(len(serie_valores)):\n",
    "            if i < ventana:\n",
    "                velocidades.append(np.nan)\n",
    "            else:\n",
    "                # Tomar los √∫ltimos n valores\n",
    "                y_values = serie_valores.iloc[i-ventana+1:i+1].values\n",
    "                x_values = np.arange(ventana)\n",
    "                \n",
    "                # Verificar que no hay NaNs o infinitos\n",
    "                \n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    if metodo == 'lineal':\n",
    "                        # Regresi√≥n lineal: pendiente = velocidad\n",
    "                        if len(y_values) > 1:\n",
    "                            slope, intercept, r_value, p_value, std_err = stats.linregress(x_values, y_values)\n",
    "                            velocidades.append(slope)\n",
    "                        else:\n",
    "                            velocidades.append(np.nan)\n",
    "                            \n",
    "                    elif metodo == 'polinomial':\n",
    "                        # Ajuste polinomial de grado 2\n",
    "                        if len(y_values) > 2:\n",
    "                            coefs = np.polyfit(x_values, y_values, 2)\n",
    "                            # La derivada de ax¬≤+bx+c es 2ax+b\n",
    "                            # En el punto final: velocidad = 2*a*(ventana-1) + b\n",
    "                            velocidad = 2 * coefs[0] * (ventana-1) + coefs[1]\n",
    "                            velocidades.append(velocidad)\n",
    "                        else:\n",
    "                            velocidades.append(np.nan)\n",
    "                            \n",
    "                    elif metodo == 'log':\n",
    "                        # Cambio logar√≠tmico (para series con crecimiento exponencial)\n",
    "                        if len(y_values) > 1 and np.all(y_values > 0):\n",
    "                            # CORRECCI√ìN 3: Verificaci√≥n m√°s robusta para valores positivos\n",
    "                            log_values = np.log(y_values)\n",
    "                            slope, _, _, _, _ = stats.linregress(x_values, log_values)\n",
    "                            velocidades.append(slope)\n",
    "                        else:\n",
    "                            velocidades.append(np.nan)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error en ventana {ventana}, posici√≥n {i}: {e}\")\n",
    "                    velocidades.append(np.nan)\n",
    "        \n",
    "        # CORRECCI√ìN 4: Verificar que las longitudes coinciden\n",
    "        if len(velocidades) != len(df_velocidad):\n",
    "            print(f\"‚ùå Error: longitud de velocidades ({len(velocidades)}) != longitud del DataFrame ({len(df_velocidad)})\")\n",
    "            # Ajustar longitud si es necesario\n",
    "            if len(velocidades) < len(df_velocidad):\n",
    "                velocidades.extend([np.nan] * (len(df_velocidad) - len(velocidades)))\n",
    "            else:\n",
    "                velocidades = velocidades[:len(df_velocidad)]\n",
    "        \n",
    "        df_velocidad[f'velocidad_{metodo}_{ventana}'] = velocidades\n",
    "        valores_validos = pd.Series(velocidades).notna().sum()\n",
    "        print(f\"  ‚úÖ Velocidad {metodo} {ventana} per√≠odos creada - {valores_validos} valores v√°lidos\")\n",
    "    \n",
    "    return df_velocidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final_no_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log(np.array([25.762100219726562,25.870071411132812,25.861082077026367])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final_no_index[\"target_diff\"] = dataset_final_no_index[\"target\"].diff()\n",
    "dataset_final_no_index = dataset_final_no_index[dataset_final_no_index[\"target_diff\"].notna()]\n",
    "train = dataset_final_no_index.loc[\"2021-01-01\":\"2023-12-31\"]\n",
    "test = dataset_final_no_index.loc[\"2024-01-01\":]\n",
    "x_train = train.drop(columns=[\"target\",\"target_diff\"])\n",
    "y_train = train[\"target_diff\"]\n",
    "x_test = test.drop(columns=[\"target\",\"target_diff\"])\n",
    "y_test = test[\"target_diff\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49838f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install catboost\n",
    "from catboost import CatBoostRegressor\n",
    "modelo_catboost = CatBoostRegressor(random_state=123, iterations=1000)\n",
    "modelo_catboost.fit(x_train, y_train)\n",
    "predicciones = modelo_catboost.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y_test, label=\"Real\")\n",
    "sns.lineplot(x = x_test.index, y = predicciones, label=\"Predicci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eacd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_base= dataset_final_no_index.loc[\"2023-12-29\",\"target\"]\n",
    "predicciones_convertidas=[]\n",
    "for i in predicciones:\n",
    "    valor_base= valor_base + i\n",
    "    predicciones_convertidas.append(valor_base)\n",
    "valor_base= dataset_final_no_index.loc[\"2023-12-29\",\"target\"]\n",
    "actual_convertidos = []\n",
    "for i in y_test:\n",
    "    valor_base= valor_base + i\n",
    "    actual_convertidos.append(valor_base)\n",
    "\n",
    "sns.lineplot(x = x_test.index, y = actual_convertidos, label=\"Real\")\n",
    "sns.lineplot(x = x_test.index, y = predicciones_convertidas, label=\"Predicci√≥n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec51520",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(modelo_catboost.feature_importances_, index=x_train.columns, columns=[\"Importancia\"]).sort_values(by=\"Importancia\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "modelo_lgbm = LGBMRegressor(random_state=123, n_estimators=4000)\n",
    "modelo_lgbm.fit(x_train.astype(float), y_train.astype(float))\n",
    "predicciones = modelo_lgbm.predict(x_test.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_base= dataset_final_no_index.loc[\"2023-12-29\",\"target\"]\n",
    "predicciones_convertidas=[]\n",
    "for i in predicciones:\n",
    "    valor_base= valor_base + i\n",
    "    predicciones_convertidas.append(valor_base)\n",
    "valor_base= dataset_final_no_index.loc[\"2023-12-29\",\"target\"]\n",
    "actual_convertidos = []\n",
    "for i in y_test:\n",
    "    valor_base= valor_base + i\n",
    "    actual_convertidos.append(valor_base)\n",
    "\n",
    "sns.lineplot(x = x_test.index, y = actual_convertidos, label=\"Real\")\n",
    "sns.lineplot(x = x_test.index, y = predicciones_convertidas, label=\"Predicci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1131116",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final_no_index[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final_no_index[\"target_30\"] = dataset_final_no_index[\"target\"].shift(-30)\n",
    "dataset_final_no_index = dataset_final_no_index[dataset_final_no_index[\"target_30\"].isna() == False]\n",
    "dataset_final_no_index[\"diff_30\"] = dataset_final_no_index[\"target\"].shift(1) - dataset_final_no_index[\"target_30\"]\n",
    "dataset_final_no_index = dataset_final_no_index[dataset_final_no_index[\"diff_30\"].isna()==False]\n",
    "\n",
    "train = dataset_final_no_index.loc[\"2021-01-01\":\"2023-12-31\"]\n",
    "test = dataset_final_no_index.loc[\"2024-01-01\":]\n",
    "x_train = train.drop(columns=[\"target\",\"target_diff\",\"target_30\",\"diff_30\"])\n",
    "y_train = train[\"target_30\"]\n",
    "x_test = test.drop(columns=[\"target\",\"target_diff\",\"target_30\",\"diff_30\"])\n",
    "y_test = test[\"target_30\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "modelo_catboost = CatBoostRegressor(random_state=123, iterations=1000)\n",
    "modelo_catboost.fit(x_train, y_train)\n",
    "predicciones = modelo_catboost.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ba04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = x_test.index, y = y_test, label=\"Real\")\n",
    "sns.lineplot(x = x_test.index, y = predicciones, label=\"Predicci√≥n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
